# -*- coding: utf-8 -*-
"""Learning Probability Density Functions using data only.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c1nzm3FzvIC5hLiqEVL4oAzpH_Au53xW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler

df = pd.read_csv("data.csv", encoding='latin1')
print(df.head())

x = df["no2"].dropna().values
print("Number of samples:", len(x))

r = 102303139
a_r = 0.5 * (r % 7)
b_r = 0.3 * ((r % 5) + 1)

print("a_r =", a_r)
print("b_r =", b_r)

z = x + a_r * np.sin(b_r * x)

print("Transformed variable 'z' shape:", z.shape)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(-1, 1))
z_scaled = scaler.fit_transform(z.reshape(-1,1))

real_data = torch.tensor(z_scaled, dtype=torch.float32)

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(1, 64),
            nn.LeakyReLU(0.2),
            nn.Linear(64, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 64),
            nn.LeakyReLU(0.2),
            nn.Linear(64, 1),
            nn.Tanh()
        )

    def forward(self, x):
        return self.model(x)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(1, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 64),
            nn.LeakyReLU(0.2),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

G = Generator()
D = Discriminator()

criterion = nn.BCELoss()

optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))

# Remove top 1% extreme values
threshold = np.percentile(z, 99)
z = z[z <= threshold]

print("New max:", np.max(z))
print("New size:", len(z))

z = np.log1p(z)   # log(1+z)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(-1,1))
z_scaled = scaler.fit_transform(z.reshape(-1,1))

real_data = torch.tensor(z_scaled, dtype=torch.float32)

epochs = 5000
batch_size = 256

for epoch in range(epochs):

    idx = np.random.randint(0, real_data.size(0), batch_size)
    real_batch = real_data[idx]

    # Label smoothing
    real_labels = torch.ones(batch_size, 1) * 0.9
    fake_labels = torch.zeros(batch_size, 1)

    # Train Discriminator
    noise = torch.randn(batch_size, 1)
    fake_data = G(noise)

    loss_real = criterion(D(real_batch), real_labels)
    loss_fake = criterion(D(fake_data.detach()), fake_labels)

    loss_D = loss_real + loss_fake

    optimizer_D.zero_grad()
    loss_D.backward()
    optimizer_D.step()

    # Train Generator
    noise = torch.randn(batch_size, 1)
    fake_data = G(noise)

    loss_G = criterion(D(fake_data), real_labels)

    optimizer_G.zero_grad()
    loss_G.backward()
    optimizer_G.step()

    if epoch % 500 == 0:
        print(f"Epoch {epoch} | Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f}")

with torch.no_grad():
    noise = torch.randn(5000, 1)
    generated = G(noise).numpy()

generated_original = scaler.inverse_transform(generated)
real_original = scaler.inverse_transform(z_scaled)

plt.figure(figsize=(10,6))
sns.kdeplot(real_original.flatten(), label="Real z", linewidth=2)
sns.kdeplot(generated_original.flatten(), label="Generated z (GAN)", linewidth=2)
plt.legend()
plt.title("Improved PDF Estimation using GAN")
plt.show()

# Number of synthetic samples
num_samples = 20000

with torch.no_grad():
    noise = torch.randn(num_samples, 1)  # Noise ~ N(0,1)
    z_fake_scaled = G(noise).numpy()

# Convert back to original scale
z_fake = scaler.inverse_transform(z_fake_scaled)

print("Generated samples shape:", z_fake.shape)

plt.figure(figsize=(10,6))

plt.hist(z_fake.flatten(), bins=50, density=True, alpha=0.6, label="Histogram PDF (GAN)")

plt.title("Histogram-based PDF Estimation from Generator Samples")
plt.xlabel("z")
plt.ylabel("Density")
plt.legend()
plt.show()
plt.savefig("histogram_pdf.png")

import seaborn as sns

plt.figure(figsize=(10,6))

sns.kdeplot(z_fake.flatten(), linewidth=3, label="KDE PDF (GAN)")
sns.kdeplot(real_original.flatten(), linewidth=3, label="Real PDF")

plt.title("KDE-based PDF Approximation from GAN Samples")
plt.xlabel("z")
plt.ylabel("Density")
plt.legend()
plt.show()
plt.savefig("KDE_pdf.png")

print("Real Mean:", np.mean(real_original))
print("Fake Mean:", np.mean(z_fake))

print("Real Variance:", np.var(real_original))
print("Fake Variance:", np.var(z_fake))

print(r)
print(a_r)
print(b_r)