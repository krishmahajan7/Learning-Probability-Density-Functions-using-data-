rollnumber(r)=102303139
a_r=0.5
b_r=1.5

Generator Architecture Description
The Generator network is designed to transform a 1-dimensional Gaussian noise input into synthetic samples of the transformed variable ğ‘§.

Structure:
Input Layer
Dimension: 1
Input distribution: Standard Gaussian noise 
ğœ–
âˆ¼
ğ‘
(
0
,
1
)
Ïµâˆ¼N(0,1)

Hidden Layer 1
Fully connected layer with 64 neurons
Activation: LeakyReLU (negative slope = 0.2)
Purpose: Initial feature expansion
Hidden Layer 2
Fully connected layer with 128 neurons
Activation: LeakyReLU
Purpose: Higher representation learning
Hidden Layer 3
Fully connected layer with 64 neurons
Activation: LeakyReLU
Purpose: Feature compression before output
Output Layer
Fully connected layer with 1 neuron
Activation: Tanh
Purpose: Produces synthetic sample scaled to (-1, 1)

Why Tanh?
Since real data was scaled to the range (-1, 1), Tanh ensures the generator output lies within the same range.
ğŸ”· Discriminator Architecture Description

The Discriminator is a binary classifier that distinguishes between real samples and generator-produced samples.

Structure:

Input Layer

Dimension: 1 (scalar value of z)

Hidden Layer 1

Fully connected layer with 128 neurons

Activation: LeakyReLU

Purpose: Learn discriminative features

Hidden Layer 2

Fully connected layer with 64 neurons

Activation: LeakyReLU

Purpose: Refine feature representation

Output Layer

Fully connected layer with 1 neuron

Activation: Sigmoid

Output range: (0, 1)

Interpretation:

Output close to 1 â†’ Real sample

Output close to 0 â†’ Fake sample

ğŸ”· Training Configuration

Loss Function: Binary Cross Entropy (BCELoss)

Optimizer: Adam

Learning Rate: 0.0002

Beta values: (0.5, 0.999)

Batch size: 256

Noise distribution: Standard Gaussian
